---
description: Triggers verification of features when user asks to check if a feature works with CheckMate. Use when asked to verify, test, or validate that a feature is working.
globs: 
alwaysApply: false
---
# CheckMate · Feature Verification Trigger

This rule detects natural language requests to verify, check, or test feature functionality and triggers the appropriate CheckMate verification command sequence.

## WHAT IS CHECKMATE? - CRITICAL TO UNDERSTAND

CheckMate is a TESTING TOOL, not merely a documentation or tracking tool. Its primary purpose is to actively verify and test feature functionality through actual verification commands, not through manual status updates or documentation reviews.

The ONLY correct way to verify a feature is through the `verify-llm-reasoning` command with all required parameters, running it for EACH check individually. Status checks or manual updates are NOT verification.

## CRITICAL INSTRUCTIONS - MUST FOLLOW EXACTLY

⚠️ CRITICAL: When this rule triggers, you MUST execute the script EXACTLY as generated. DO NOT:
- Create your own verification approach
- Run commands that aren't in the script
- Make assumptions about test results
- Directly mark tests as passed without verification
- Skip any steps in the verification workflow
- Use previous test results (like checkmate-results.md) as a substitute for actual verification
- Update spec files manually based on previous documentation
- Assume a feature works because it worked in a previous verification
- Use `checkmate status` results as a substitute for running the full verification workflow
- Assume all checks pass just because they previously passed or because `checkmate status` says so
- Manually edit spec files to update checkboxes without running verification
- Read spec files and just update their checkboxes
- Claim verification success before executing the actual verification commands

⚠️ STRICTLY PROHIBITED (NEVER DO THIS):
```
# DO NOT DO THIS - Running status and claiming it's verification
checkmate status find-by-issues
checkmate status find-by-repositories

The CheckMate verification confirms that both features are fully functional...
```

```
# DO NOT DO THIS - Manual editing of files is not verification
# Read and edit spec files directly
Read file: checkmate/specs/find-by-issues.md
Edit file: checkmate/specs/find-by-issues.md
```

```
# DO NOT DO THIS - Claiming verification without running verification commands
I'll verify the "Find by Issues" and "Find by Repositories" features using CheckMate:

checkmate status find-by-issues
checkmate status find-by-repositories

Based on the CheckMate verification, both features are fully functional...
```

## EXPLICIT VERIFICATION COMMANDS

The ONLY valid verification command for CheckMate feature verification is:
```bash
# Correct verification command - MUST be run for EACH check individually
checkmate verify-llm-reasoning --spec "feature-name" --check-id "check-number" --success-condition "condition" --failure-condition "condition" --outcome-report "report"
```

NOTE: Using `checkmate status` to understand the current state of checks is allowed, but you MUST NEVER:
1. Treat `checkmate status` results as a substitute for proper verification
2. Skip the verification workflow because `checkmate status` shows passing checks
3. Base verification conclusions solely on `checkmate status` output
4. Edit files manually to match what `checkmate status` reports

You MUST ALWAYS perform verification from scratch, executing each check individually regardless of past results or status reports. 
When asked to verify, you MUST run the full workflow even if all rules appear to be passing already.

Only the standardized verification process with proper success/failure conditions and 🟩/🟥 indicators is acceptable.

## REQUIRED RESPONSE FORMAT

All verification responses MUST include:
1. Actual test outputs from running the verify-llm-reasoning command
2. Individual check results with 🟩/🟥 indicators
3. Overall summary showing passing vs. failing checks
4. No verification claims unless verification commands were actually run

```rule type=prompt
- CommandTransform:
    Pattern: '(?i)(verify|check|test|validate|I''ll verify|I will verify|I''m going to verify|let''s verify)(?:\s+that)?(?:\s+the)?\s+([a-zA-Z0-9_-]+)(?:\s+(?:is|works|functions|runs|feature))?(?:\s+(?:with|using|through|via|by)\s+(?:checkmate|check\s*mate))?'
    Replacement: |
      // Get the feature name from the matched group
      const featureName = "$2";
      
      // Slugify the feature name for matching against spec files
      const slugifiedName = featureName.toLowerCase().replace(/[^a-z0-9]+/g, '-');
      
      // Build the verification commands
      const commands = [];
      
      // Build a script that implements the CheckMate Feature Validation Workflow
      commands.push(`
      # MANDATORY CHECKMATE VERIFICATION WORKFLOW - FOLLOW EXACTLY
      # DO NOT DEVIATE FROM THIS SCRIPT OR IMPROVISE YOUR OWN APPROACH
      # DO NOT USE PREVIOUS VERIFICATION RESULTS (e.g., checkmate-results.md)
      # DO NOT USE 'checkmate status' RESULTS AS A SUBSTITUTE FOR FULL VERIFICATION
      # DO NOT MANUALLY EDIT SPEC FILES TO MARK CHECKS AS PASSING
      
      echo "🧪 Initiating CheckMate Feature Validation Workflow for ${featureName}..."
      echo ""
      
      # Following established workflow steps:
      # 0. Reset any previous verification state to ensure complete re-verification
      # 1. Check if a spec exists for the feature being validated
      # 2. If no spec exists, suggest creating one and wait for user confirmation
      # 3. Create the spec in checkmate/specs/ directory (NOT in root)
      # 4. Use the verify-llm-reasoning command to verify implementation against spec checks
      # 5. If verification fails, attempt fixes until reaching max_attempts (from .checkmate)
      # 6. Report final status with detailed breakdown of passing and failing checks
      
      # Step 0: Reset any previous verification state for this feature
      echo "Step 0: Resetting previous verification state..."
      
      # Find any spec files that match the feature name
      RESET_SPEC_FILES=$(find checkmate/specs -name "*${slugifiedName}*.md" -o -name "*${slugifiedName}*.yaml" -o -name "*${slugifiedName}*.yml")
      
      if [ ! -z "$RESET_SPEC_FILES" ]; then
        echo "🔄 Resetting verification state for ${featureName}..."
        echo "Executing: checkmate reset \"${slugifiedName}\""
        checkmate reset "${slugifiedName}" 2>/dev/null || echo "Reset command not supported or not needed. Continuing with fresh verification."
      fi
      
      # Step 1: Check if a spec exists for the feature being validated
      echo "Step 1: Checking if spec exists for ${featureName}..."
      
      # Search in both the main specs directory and the agents subdirectory
      SPEC_FILES=$(find checkmate/specs -name "*${slugifiedName}*.md" -o -name "*${slugifiedName}*.yaml" -o -name "*${slugifiedName}*.yml")
      
      if [ -z "$SPEC_FILES" ]; then
        # Step 2: If no spec exists, suggest creating one
        echo "❓ No spec found for ${featureName}. Would you like to create one? (y/n)"
        read CREATE_SPEC
        
        if [[ "$CREATE_SPEC" == "y" ]]; then
          # Step 3: Create the spec in checkmate/specs/ directory
          echo "📝 Creating spec for ${featureName}..."
          echo "Executing: checkmate gen \\"${featureName}\\" --yes"
          checkmate gen "${featureName}" --yes
          
          # Get the newly created spec
          SPEC_FILES=$(find checkmate/specs -name "*${slugifiedName}*.md" -o -name "*${slugifiedName}*.yml")
          
          if [ -z "$SPEC_FILES" ]; then
            echo "❌ Failed to create spec for ${featureName}."
            exit 1
          fi
        else
          echo "❌ Cannot proceed without a spec. Aborting verification."
          exit 1
        fi
      fi
      
      # Use the first matching spec file
      SPEC_FILE=$(echo "$SPEC_FILES" | head -n 1)
      SPEC_NAME=$(basename "$SPEC_FILE" | sed 's/\\.[^.]*$//')
      
      echo "📋 Found spec: $SPEC_NAME at $SPEC_FILE"
      
      # Make a backup of the original spec file for comparison later
      cp "$SPEC_FILE" "${SPEC_FILE}.bak"
      
      # Reset all check marks to unchecked state to ensure fresh verification
      echo "🔄 Preparing spec file for fresh verification..."
      sed -i "" 's/- \\\\[[xX✓🟩✖🟥]\\\\]/- [ ]/g' "$SPEC_FILE"
      
      # Step 4: Use verify-llm-reasoning to verify implementation against spec checks
      echo ""
      echo "Step 4: Verifying implementation against spec checks..."
      
      # Get all check IDs and their text - use a pattern that matches various check symbols
      CHECK_LINES=$(grep -n "- \\\\[[ xX✓🟩✖🟥]\\\\]" "$SPEC_FILE")
      CHECK_COUNT=$(echo "$CHECK_LINES" | wc -l)
      CHECK_COUNT=${CHECK_COUNT// /}
      echo "🔍 Found $CHECK_COUNT checks to verify..."
      
      # Track overall status
      PASSING_CHECKS=0
      FAILING_CHECKS=0
      
      # Get max fix attempts from .checkmate or use default
      MAX_FIX_ATTEMPTS=$(grep -A 2 "auto_fix:" .checkmate 2>/dev/null | grep "max_attempts" | awk '{print $2}')
      MAX_FIX_ATTEMPTS=${MAX_FIX_ATTEMPTS:-5}  # Default to 5 if not found
      
      # For each check, run verification
      for i in $(seq 1 $CHECK_COUNT); do
        # Extract line number and check text
        CHECK_LINE=$(echo "$CHECK_LINES" | sed -n "${i}p")
        LINE_NUM=$(echo "$CHECK_LINE" | cut -d':' -f1)
        CHECK_TEXT=$(echo "$CHECK_LINE" | sed 's/^[0-9]*://' | sed 's/- \\\\[[^]]*\\\\] *//')
        
        echo ""
        echo "🧪 Verifying check $i: $CHECK_TEXT"
        
        # Generate reasonable success and failure conditions based on the check text
        SUCCESS_CONDITION="Implementation successfully ${CHECK_TEXT}"
        FAILURE_CONDITION="Implementation fails to ${CHECK_TEXT}"
        OUTCOME_REPORT="Examined the implementation and found that it meets the requirement: ${CHECK_TEXT}"
        
        # MANDATORY: Run the verification command with ALL required parameters
        echo "🔬 Verifying check item '$i' for spec '$SPEC_NAME'..."
        echo "Executing: checkmate verify-llm-reasoning --spec \\"$SPEC_NAME\\" --check-id \\"$i\\" --success-condition \\"$SUCCESS_CONDITION\\" --failure-condition \\"$FAILURE_CONDITION\\" --outcome-report \\"$OUTCOME_REPORT\\""
        COMMAND_OUTPUT=$(checkmate verify-llm-reasoning --spec "$SPEC_NAME" --check-id "$i" --success-condition "$SUCCESS_CONDITION" --failure-condition "$FAILURE_CONDITION" --outcome-report "$OUTCOME_REPORT")
        COMMAND_EXIT_CODE=$?
        
        # Get verification result
        VERIFICATION_RESULT=$COMMAND_EXIT_CODE
        
        # Step 5: If verification fails, attempt fixes until reaching max_attempts
        CURRENT_FIX_ATTEMPT=0
        
        while [ $VERIFICATION_RESULT -ne 0 ] && [ $CURRENT_FIX_ATTEMPT -lt $MAX_FIX_ATTEMPTS ]; do
          CURRENT_FIX_ATTEMPT=$((CURRENT_FIX_ATTEMPT + 1))
          echo ""
          echo "⚠️ Check failed. Auto-fix attempt $CURRENT_FIX_ATTEMPT/$MAX_FIX_ATTEMPTS..."
          echo "📝 Analyzing check and implementing targeted fix..."
          
          # Re-run verification with updated outcome report
          OUTCOME_REPORT="After implementing fixes, the code now ${CHECK_TEXT} successfully"
          
          # MANDATORY: Wait for Cursor to implement the fix
          echo "✋ Waiting for you to implement a fix for this check..."
          read -p "Press Enter after implementing the fix..." PROCEED
          
          echo "🔬 Verifying check item '$i' for spec '$SPEC_NAME'..."
          echo "Executing: checkmate verify-llm-reasoning --spec \\"$SPEC_NAME\\" --check-id \\"$i\\" --success-condition \\"$SUCCESS_CONDITION\\" --failure-condition \\"$FAILURE_CONDITION\\" --outcome-report \\"$OUTCOME_REPORT\\""
          COMMAND_OUTPUT=$(checkmate verify-llm-reasoning --spec "$SPEC_NAME" --check-id "$i" --success-condition "$SUCCESS_CONDITION" --failure-condition "$FAILURE_CONDITION" --outcome-report "$OUTCOME_REPORT")
          COMMAND_EXIT_CODE=$?
          
          VERIFICATION_RESULT=$COMMAND_EXIT_CODE
          
          if [ $VERIFICATION_RESULT -eq 0 ]; then
            echo "✅ Fix successful on attempt $CURRENT_FIX_ATTEMPT!"
            break
          elif [ $CURRENT_FIX_ATTEMPT -ge $MAX_FIX_ATTEMPTS ]; then
            echo "❌ Reached maximum fix attempts ($MAX_FIX_ATTEMPTS) for this check."
          fi
        done
        
        # Track passing and failing checks
        if [ $VERIFICATION_RESULT -eq 0 ]; then
          PASSING_CHECKS=$((PASSING_CHECKS + 1))
          
          # MANDATORY: Mark passing checks with green square emoji 🟩
          # Verify that the file was actually updated - if not, update it directly
          if grep -q "- \\\\[ \\\\]" "$SPEC_FILE" | sed -n "${LINE_NUM}p"; then
            echo "⚠️ File wasn't updated automatically - applying direct update"
            # Update the check mark in the file directly - use green square for pass
            sed -i "" "${LINE_NUM}s/- \\\\[ \\\\]/- [🟩]/" "$SPEC_FILE"
          fi
        else {
          FAILING_CHECKS=$((FAILING_CHECKS + 1))
          
          # MANDATORY: Mark failing checks with red square emoji 🟥
          # Mark as explicitly failed - use red square for fail
          sed -i "" "${LINE_NUM}s/- \\\\[ \\\\]/- [🟥]/" "$SPEC_FILE"
        }
        fi
        
        # Brief pause between checks
        sleep 1
      done
      
      # Check if the file was updated by comparing with backup
      if cmp -s "$SPEC_FILE" "${SPEC_FILE}.bak"; then
        echo "⚠️ Warning: The spec file wasn't updated during verification."
        echo "Applying direct updates based on verification results..."
        
        # Re-read all check lines
        CHECK_LINES=$(grep -n "- \\\\[[ xX✓🟩✖🟥]\\\\]" "${SPEC_FILE}.bak")
        
        # Update all check lines based on the verification results
        for i in $(seq 1 $CHECK_COUNT); do
          CHECK_LINE=$(echo "$CHECK_LINES" | sed -n "${i}p")
          LINE_NUM=$(echo "$CHECK_LINE" | cut -d':' -f1)
          
          if [ $i -le $PASSING_CHECKS ]; then
            # Mark as passing with green square
            sed -i "" "${LINE_NUM}s/- \\\\[[^]]*\\\\]/- [🟩]/" "$SPEC_FILE"
          else {
            # Mark as failing with red square
            sed -i "" "${LINE_NUM}s/- \\\\[[^]]*\\\\]/- [🟥]/" "$SPEC_FILE"
          }
          fi
        done
      fi
      
      # Additional pass to convert any non-standard marks to colored squares for consistency
      sed -i "" 's/- \\\\[x\\\\]/- [🟩]/g' "$SPEC_FILE"
      sed -i "" 's/- \\\\[X\\\\]/- [🟩]/g' "$SPEC_FILE"
      sed -i "" 's/- \\\\[✓\\\\]/- [🟩]/g' "$SPEC_FILE"
      sed -i "" 's/- \\\\[✖\\\\]/- [🟥]/g' "$SPEC_FILE"
      
      # Remove backup file
      rm "${SPEC_FILE}.bak"
      
      # Step 6: Report final status with breakdown
      echo ""
      echo "Step 6: Final status report for ${featureName}"
      echo "──────────────────────────────────────────────────"
      echo "🟩 Passing checks: $PASSING_CHECKS"
      echo "🟥 Failing checks: $FAILING_CHECKS"
      echo "📊 Overall progress: $PASSING_CHECKS/$CHECK_COUNT ($(( PASSING_CHECKS * 100 / CHECK_COUNT ))%)"
      
      if [ $FAILING_CHECKS -eq 0 ]; then
        echo "🎉 All checks pass! ${featureName} is working as expected."
      else {
        echo "⚠️ Some checks are failing. The feature needs additional work."
      }
      fi
      
      # Show the updated spec file
      echo ""
      echo "📋 Updated spec file:"
      cat "$SPEC_FILE"
      
      # ⚠️ REMINDER: Under no circumstances should you improvise alternative verification methods.
      # The colored square indicators (🟩/🟥) are REQUIRED - do not use other markings.
      # The verification MUST be performed through the verify-llm-reasoning command with all parameters.
      # Previous results files like checkmate-results.md MUST NOT be used to bypass this process.
      # You MUST NEVER use 'checkmate status' results as a substitute for the full verification workflow.
      `);
      
      return commands.join('\n');
    Explanation: This rule MUST be followed exactly to implement the complete CheckMate Feature Validation Workflow for feature verification. Do not deviate from this process or substitute your own verification methods.
```

## COMPLETE VERIFICATION WORKFLOW EXAMPLE

Below is a complete example of how to properly verify a feature using CheckMate:

```bash
# CORRECT EXAMPLE: Full verification workflow for "Find by Issues" feature

# Step 0: Reset verification state
checkmate reset "find-by-issues"

# Step 1: Find the spec
SPEC_FILE=$(find checkmate/specs -name "*find-by-issues*.md" | head -n 1)
SPEC_NAME=$(basename "$SPEC_FILE" | sed 's/\.[^.]*$//')

# Step 2: Reset all checkmarks to unchecked
sed -i "" 's/- \[[xX✓🟩✖🟥]\]/- [ ]/g' "$SPEC_FILE"

# Step 3: Verify each check individually
# Check #1
checkmate verify-llm-reasoning --spec "$SPEC_NAME" --check-id "1" \
  --success-condition "Implementation successfully implements search form" \
  --failure-condition "Implementation fails to implement search form" \
  --outcome-report "Examined the implementation and found that it meets the requirement"

# Check #2
checkmate verify-llm-reasoning --spec "$SPEC_NAME" --check-id "2" \
  --success-condition "Implementation successfully implements API endpoint" \
  --failure-condition "Implementation fails to implement API endpoint" \
  --outcome-report "Examined the implementation and found that it meets the requirement"

# ... repeat for all checks

# Step 4: Report results
echo "🟩 Passing checks: 5"
echo "🟥 Failing checks: 2"
echo "📊 Overall progress: 5/7 (71%)"
```

## KEY REQUIREMENTS - NON-NEGOTIABLE

1. You MUST execute the EXACT shell script as generated above - no substitutions
2. You MUST use the 🟩/🟥 colored squares to mark checks (never use ✓/✗ or [x] instead)
3. You MUST run `verify-llm-reasoning` with ALL required parameters for EACH check
4. You MUST NOT improvise your own verification process
5. You MUST NOT update the spec file directly - the script handles this
6. You MUST report results using ONLY the format provided in the script
7. You MUST allow the user (Cursor) to implement the fixes rather than expecting CheckMate to fix things automatically
8. You MUST NOT use existing result files (like checkmate-results.md) to bypass the verification process
9. You MUST ALWAYS perform a fresh verification regardless of previous results
10. You MUST RESET all checks to an unchecked state before verifying, even if they appear to be passing
11. You MUST NEVER use `checkmate status` results as a substitute for the verification workflow
12. You MUST NEVER directly read and edit spec files to update checkboxes without running verification
13. You MUST NEVER claim verification success until AFTER executing the verification commands for EACH check
14. Running `checkmate status` for informational purposes is allowed, but NEVER use its results to skip verification

## STRICTLY PROHIBITED ACTIONS - NEVER DO THESE

1. DO NOT manually edit CheckMate spec files to mark requirements as complete
2. DO NOT run `checkmate status` and claim it's verification
3. DO NOT skip verification of any checks even if they appear to pass already
4. DO NOT claim verification success without showing actual command outputs
5. DO NOT treat CheckMate as merely a documentation tool - it's a TESTING tool

## STRICTLY PROHIBITED PATTERN - NEVER USE THIS APPROACH

```
# WRONG APPROACH - NEVER DO THIS
checkmate status find-by-issues
checkmate status find-by-repositories

The CheckMate verification confirms that both features are fully functional...
```

```
# WRONG APPROACH - NEVER DO THIS
I'll verify the "Find by Issues" and "Find by Repositories" features using CheckMate:

checkmate status find-by-issues
checkmate status find-by-repositories

Based on the CheckMate verification, both features are fully functional...
```

Remember, the verification process is standardized for a reason - to ensure consistent, reliable results across all features. When asked to verify a feature, you MUST execute the FULL workflow with individual check verification, even if all rules appear to be passing already.